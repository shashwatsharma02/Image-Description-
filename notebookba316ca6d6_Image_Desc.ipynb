{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7664861,"sourceType":"datasetVersion","datasetId":4469873}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code uses llama-2 13 B model to create description of given image. it starts by pre processing the image then uses pytessearct to perform Optical Character recognition and that extracted text is passed to llama-2-13B using a prompt template.","metadata":{}},{"cell_type":"markdown","source":"# Importing image and pre processing it to extract the test description out of it using pyTesseract","metadata":{}},{"cell_type":"code","source":"import cv2\nimport pytesseract\nfrom torchvision import transforms\nimport torch\nimport numpy as np\nfrom PIL import Image\n\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    \n    if image is None:\n        raise ValueError(\"Error loading image at path:\", image_path)\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    if len(image.shape) != 3 or image.shape[2] != 3:\n        raise ValueError(\"Invalid image shape:\", image.shape)\n    \n    image_pil = Image.fromarray(image)\n    \n    if image_pil.mode != 'RGB':\n        raise ValueError(\"Unsupported image mode:\", image_pil.mode)\n    \n    return image_pil\n\ndef detect_text(image):\n    text = pytesseract.image_to_string(image)\n    return text\n\n\ndef generate_description(text):\n    description = \"Text in the image: \" + text + \". \"\n    return description\n\ndef generate_natural_language_description(image_path):\n    image = preprocess_image(image_path)\n    text = detect_text(image)\n    description = generate_description(text)\n    return description\n\nimage_path = \"/kaggle/input/describe-image/WhatsApp Image 2024-02-20 at 9.21.21 PM.jpeg\"\ndescription = generate_natural_language_description(image_path)\nprint(description)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-20T16:29:29.007817Z","iopub.execute_input":"2024-02-20T16:29:29.008201Z","iopub.status.idle":"2024-02-20T16:29:30.031911Z","shell.execute_reply.started":"2024-02-20T16:29:29.008173Z","shell.execute_reply":"2024-02-20T16:29:30.030970Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Text in the image: ACL)\n\nFET 01 eo) as Se\n\n \n\nThe IndianEXPRESS [RPSubseibe) my account\n\nJOURNALISM OF COURAGE\n\nHome Cities India Explained Opinion Business E\n\nUPSC Pack Play Crossword Express Shorts © F\n\nNews / Education / IIM-Calcutta achieves 100% place\n\nIIM-Calcutta achieves 100%\nplacement with 529 offers\n\nIIM Placements: This year witnessed a new\ntrend of firms offering role in ‘Founder’s\noffice’. These roles were offered by OLA,\nIndusInd Bank, Newme, ParkStreet, PoshN,\nKofluence among others\n\nADVERTISEMENT\n\nCar insurance\npolicy st\n\n= 2094\n\n‘Asta Secure -Provate Car Package Peicy- UIN = TRDANUHRIGOR2¥01 206081\n\n \n\nEXPRESS OPINION C Raja Mohan writes: The rise of Xx\n‘intelligence diplomacy’ in a time of global security\nchallenges\n\f. \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing necessary libraries to run llama2","metadata":{}},{"cell_type":"code","source":"!pip install \"transformers[sentencepiece]==4.32.1\" \"optimum==1.12.0\" \"auto-gptq==0.4.2\" \"accelerate==0.22.0\" \"safetensors>=0.3.1\" --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-02-20T16:00:58.693060Z","iopub.execute_input":"2024-02-20T16:00:58.693828Z","iopub.status.idle":"2024-02-20T16:01:24.416481Z","shell.execute_reply.started":"2024-02-20T16:00:58.693793Z","shell.execute_reply":"2024-02-20T16:01:24.415499Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting transformers==4.32.1 (from transformers[sentencepiece]==4.32.1)\n  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting optimum==1.12.0\n  Downloading optimum-1.12.0-py3-none-any.whl.metadata (16 kB)\nCollecting auto-gptq==0.4.2\n  Downloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting accelerate==0.22.0\n  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.1->transformers[sentencepiece]==4.32.1)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.1->transformers[sentencepiece]==4.32.1) (4.66.1)\nCollecting coloredlogs (from optimum==1.12.0)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.12.0) (1.12)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from optimum==1.12.0) (2.1.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.12.0) (2.1.0)\nCollecting rouge (from auto-gptq==0.4.2)\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nCollecting peft (from auto-gptq==0.4.2)\n  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.32.1) (0.1.99)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.32.1) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->optimum==1.12.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->optimum==1.12.0) (3.1.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.12.0)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (0.70.15)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.12.0) (0.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.1->transformers[sentencepiece]==4.32.1) (2023.11.17)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq==0.4.2) (1.16.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.12.0) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.12.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.12.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.12.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.12.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.12.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.12.0) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->optimum==1.12.0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.12.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.12.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.12.0) (2023.4)\nDownloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading optimum-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.6/380.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, rouge, humanfriendly, coloredlogs, transformers, accelerate, peft, optimum, auto-gptq\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.1\n    Uninstalling tokenizers-0.15.1:\n      Successfully uninstalled tokenizers-0.15.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.26.1\n    Uninstalling accelerate-0.26.1:\n      Successfully uninstalled accelerate-0.26.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.32.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.22.0 auto-gptq-0.4.2 coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.12.0 peft-0.8.2 rouge-1.0.1 tokenizers-0.13.3 transformers-4.32.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing base llama-2 13B model from GPT-Q","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport time\n\nmodel_name_or_path = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\nmodel_basename = \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\n\n\n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T16:01:24.418167Z","iopub.execute_input":"2024-02-20T16:01:24.418438Z","iopub.status.idle":"2024-02-20T16:02:13.962725Z","shell.execute_reply.started":"2024-02-20T16:01:24.418412Z","shell.execute_reply":"2024-02-20T16:02:13.961883Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-20 16:01:26.523754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-20 16:01:26.523855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-20 16:01:26.651672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31da0f2203a14e8ca5b8d47973f25348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8a5ec3894a54fe29c36504b40fb0691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92936537bca1425b9e727b42c8c78058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ebbfebff76431dad751ef861523e4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b69e7f46464e4138b3614b796c25f8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"quantize_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75d367d592c04b20a94176c6f115b4d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95fb50799aa74b89b8558d3d9168b52d"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Defining prompt template for usage","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nprompt = description\nsystem_message = \"You are a helpful, respectful and honest assistant. Given a description of an images try to write a more detailed and insightful descrption of that text in that image. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\nprompt_template=f'''[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{prompt} [/INST]'''\n\nresponse = pipe(prompt_template)\nend = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T16:29:39.571280Z","iopub.execute_input":"2024-02-20T16:29:39.572168Z","iopub.status.idle":"2024-02-20T16:36:34.602376Z","shell.execute_reply.started":"2024-02-20T16:29:39.572124Z","shell.execute_reply":"2024-02-20T16:36:34.601569Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Using the model to get descrpition of image\n","metadata":{}},{"cell_type":"code","source":"print(response[0][\"generated_text\"].split(\"[/INST]\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-02-20T16:36:34.603883Z","iopub.execute_input":"2024-02-20T16:36:34.604193Z","iopub.status.idle":"2024-02-20T16:36:34.609181Z","shell.execute_reply.started":"2024-02-20T16:36:34.604167Z","shell.execute_reply":"2024-02-20T16:36:34.608251Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"  Sure! Here is a more detailed and insightful description of the text in the image:\n\nThe text appears to be a portion of a newspaper article or online news piece, specifically the \"OPINION\" section. The headline reads \"JOURNALISM OF COURAGE,\" and the subheading below it says \"Home Cities India Explained Opinion Business E.\"\n\nThe text discusses the placement trends at the Indian Institute of Management (IIM) in Calcutta, where all students have been placed with 529 offers. It highlights a new trend this year of firms offering \"role in 'Founder's office'\" to students, which suggests that these companies are looking for individuals who can take on leadership positions and contribute to the growth of their businesses. Some of the companies mentioned in the text that have offered such roles include OLA, IndusInd Bank, Newme, ParkStreet, PoshN, and Kofluence.\n\nThe text also includes an advertisement for car insurance policy, with the headline \"Asta Secure - Private Car Package Policy - UIN = TRDANUHRIGOR2¥01 206081.\"\n\nOverall, the text seems to be discussing current events and issues related to education and employment in India, particularly the placement trends at IIM-Calcutta and the emergence of \"intelligence diplomacy\" in the context of global security challenges.\n","output_type":"stream"}]}]}